import numpy as np

from TinyDL.core.autograd.function import Function
from TinyDL.core.autograd.variable import Variable
from TinyDL.core.autograd.utils import DyGraphTracer
from TinyDL.core.operators.common import * 
from TinyDL.core.types import Vcpu, Vcuda, V

class Tensor(Variable):
    """
    # Tensor

    ## Attributes
            parent class (Variable) attributes ...
            tracer (DyGraphTracer): track the structure of computation graph

    ## Args
            data (V): data that stored in this tensor
            requires_grad (bool): need to compute gradient for this tensor
            device (str): cpu or cuda device
            name (str): tensor name
            tracer (DyGraphTracer): track the structure of computation graph
    """
    def __init__(self, 
        data: V, 
        requires_grad: bool = False, 
        device: str = 'cpu',
        name: str = "",
        tracer: DyGraphTracer = None) -> None:

        super().__init__(data, requires_grad, device, name)

        self.tracer: DyGraphTracer = tracer
        if self.tracer is not None and self.is_leaf:
            tracer.step(self)

    def _wrap_tensor(self, x: Variable, y: Variable, operator: Function, param=None):
        """
        accept V type input and use operator to generate output, finally,
        wrap this output into Tensor and add to computation graph 
        """
        if y is not None:
            y = y if isinstance(y, Variable) else Tensor(
                data=y,
                device=self.device
            )
        out = operator.forward(x.data, y.data if y is not None else None, param)
        # whether a cache is generated by operator 
        cache = None
        if isinstance(out, tuple):
            data, cache = out
        else:
            data = out
        # create output tensor
        new_tensor = Tensor(
            data=data,
            requires_grad=self.requires_grad,
            device=x.device
        )
        # if a cache is generated, save cache in output tensor
        if cache is not None:
            new_tensor.set_cache(cache)
        new_tensor = self._add_to_graph(new_tensor, x, y, operator, param)
        if self.tracer is not None:
            new_tensor.tracer = self.tracer
            self.tracer.step(new_tensor)
        return new_tensor

    def set_name(self, name: str):
        self.name = name

    def set_tracer(self, tracer: DyGraphTracer):
        self.tracer = tracer
        if self.is_leaf:
            self.tracer.step(self)

    def set_cache(self, cache):
        self.cache = cache

    def shape(self):
        if isinstance(self.data, np.ndarray):
            return self.data.shape
        else:
            return [1]

    def zero_grad(self):
        """
        clear the gradient of this tensor
        """
        if self.grad is not None:
            self.grad = self._constant_like_data(fill=0)

    def __add__(self, x):
        return self._wrap_tensor(self, x, Add)

    def __sub__(self, x):
        return self._wrap_tensor(self, x, Sub)

    def __neg__(self):
        return self._wrap_tensor(self, None, Neg)

    def __mul__(self, x):
        return self._wrap_tensor(self, x, Mul)

    def __truediv__(self, x):
        return self._wrap_tensor(self, x, Div)

    def __pow__(self, p: float):
        return self._wrap_tensor(self, None, Pow, {'power': p})

    def matmul(self, x):
        return self._wrap_tensor(self, x, Matmul)

    def T(self):
        return self._wrap_tensor(self, None, Transpose)

    def permute(self, *args):
        return self._wrap_tensor(self, None, Permute, {'axis': tuple(args)})



        


